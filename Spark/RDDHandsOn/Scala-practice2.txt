
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

Downloads data from www.healthdata.gov

scala> val parallel = sc.parallelize(1 to 9)parallel: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[96] at parallelize at <console>:24//mapPartitions用了iterator 过一遍List里面的元素scala> parallel.mapPartitions(x => List(x.next).iterator).collectres29: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8)scala> val parallel = sc.parallelize(1 to 9)parallel: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[100] at parallelize at <console>:24scala> parallel.sample(true,.2).countres31: Long = 1scala> parallel.sample(true,.2).countres32: Long = 1scala> parallel.sample(true,.2).countres33: Long = 2scala> parallel.sample(true,.2).countres34: Long = 0scala> val l1 = sc.parallelize(1 to 9)l1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[112] at parallelize at <console>:24scala> val l2 = sc.parallelize(5 to 10)l3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[115] at parallelize at <console>:24scala> l1.union(l2).collectres46: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9, 10)scala> l1.intersection(l2).collectres47: Array[Int] = Array(8, 9, 5, 6, 7)scala> l1.union(l2).distinct.collectres49: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala> l1.union(l2).distinct.countres48: Long = 10//Spark-key：scala> val babyNames = sc.textFile("baby_names.csv")babyNames: org.apache.spark.rdd.RDD[String] = baby_names.csv MapPartitionsRDD[133] at textFile at <console>:24scala> val rows = babyNames.map(line => line.split(","))rows: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[134] at map at <console>:26//Use map to create keyscala> val namesToCountries = rows.map(name => (name(1),name(2)))namesToCountries: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[135] at map at <console>:28scala> namesToCountries.groupByKey.collectres50: Array[(String, Iterable[String])] = Array((BRADEN,CompactBuffer(SUFFOLK, SARATOGA, SUFFOLK, ERIE, SUFFOLK, SUFFOLK, ERIE)), (MATTEO,CompactBuffer(NEW YORK, SUFFOLK, NASSAU, KINGS, WESTCHESTER, WESTCHESTER, KINGS, SUFFOLK, NASSAU, QUEENS, QUEENS, NEW YORK, NASSAU, QUEENS, KINGS, SUFFOLK, WESTCHESTER, WESTCHESTER, SUFFOLK, KINGS, NASSAU, QUEENS, SUFFOLK, NASSAU, WESTCHESTER)), (HAZEL,CompactBuffer(ERIE, MONROE, KINGS, NEW YORK, KINGS, MONROE, NASSAU, SUFFOLK, QUEENS, KINGS, SUFFOLK, NEW YORK, KINGS, SUFFOLK)), (SKYE,CompactBuffer(NASSAU, KINGS, MONROE, BRONX, KINGS, KINGS, NASSAU)), (JOSUE,CompactBuffer(SUFFOLK, NASSAU, WESTCHESTER, BRONX, KINGS, QUEENS, SUFFOLK, QUEENS, NASSAU, WESTCHESTER, BRONX, BRONX, QUEENS, SUFFOLK, KINGS, WESTCHESTER, QUEENS, NASSAU, SUFFOLK, BRONX, KINGS, Q...scala> filteredRow.map(n => (n(1), n(4).toInt)).reduceByKey((a,b) => a + b).collectres51: Array[(String, Int)] = Array((BRADEN,39), (MATTEO,279), (HAZEL,133), (RORY,12), (SKYE,63), (JOSUE,404), (NAHLA,16), (ASIA,6), (HINDY,254), (ELVIN,26), (MEGAN,581), (AMARA,10), (BELLA,672), (DANTE,246), (CHARLOTTE,1737), (EPHRAIM,26), (PAUL,712), (ANGIE,295), (ANNABELLA,38), (DIAMOND,16), (ALFONSO,6), (MELISSA,560), (AYANNA,11), (ANIYAH,365), (DINAH,5), (MARLEY,32), (OLIVIA,6467), (MALLORY,15), (EZEQUIEL,13), (ELAINE,116), (ESMERALDA,71), (SKYLA,172), (EDEN,199), (MEGHAN,128), (AHRON,29), (KINLEY,5), (RUSSELL,5), (TROY,88), (JALIYAH,10), (MORDECHAI,521), (AUDREY,690), (VALERIE,584), (JAYSON,285), (SKYLER,26), (DASHIELL,24), (SHAINDEL,17), (ANGELY,5), (AURORA,86), (ANDERSON,369), (SHMUEL,315), (MARCO,370), (AUSTIN,1345), (MITCHELL,12), (SELINA,187), (FATIMA,421), (CESAR,292), (CARI...scala> filteredRows.map(n => (n(1),n(4))).sortByKey() > resutl.txt//joinscala> val name1 = sc.parallelize(List("abe","abby","apple")).map(a => (a,1))name1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[146] at map at <console>:24scala> val name2 = sc.parallelize(List("apple","beatty","beaty")).map(a => (a,1))name2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[148] at map at <console>:24scala> name1.join(name2).collectres56: Array[(String, (Int, Int))] = Array((apple,(1,1)))scala> name1.leftOuterJoin(name2).collectres60: Array[(String, (Int, Option[Int]))] = Array((abby,(1,None)), (apple,(1,Some(1))), (abe,(1,None)))scala> name1.rightOuterJoin(name2).collectres63: Array[(String, (Option[Int], Int))] = Array((apple,(Some(1),1)), (beatty,(None,1)), (beaty,(None,1)))//extra levelscala> name1.join(name2).join(name2).collectres57: Array[(String, ((Int, Int), Int))] = Array((apple,((1,1),1)))//repeated “Apple” Join cannot remove duplicates.scala> val name3 = sc.parallelize(List("abe","apple","apple")).map(a => (a,1))name3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[159] at map at <console>:24scala> name3.join(name1).collectres58: Array[(String, (Int, Int))] = Array((apple,(1,1)), (apple,(1,1)), (abe,(1,1)))scala> name1.leftOuterJoin(name3).collectres61: Array[(String, (Int, Option[Int]))] = Array((abby,(1,None)), (apple,(1,Some(1))), (apple,(1,Some(1))), (abe,(1,Some(1))))//Action:reduce (will auto sort before concating)scala> val name1 = sc.parallelize(List("IBM", "Apple", "Yahoo"))name1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[175] at parallelize at <console>:24scala> name1.reduce((a,b) => a + b)res64: String = AppleIBMYahooscala> val word = sc.parallelize(List("D", "u","c","k"))word: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[176] at parallelize at <console>:24scala> word.reduce((a,b) => a + b)res65: String = cuDk
//Attention: reduce is action which will be the end of the command
//flatMap is transformation which generates RDD, so need to put forward.scala> name1.reduce((a,b) => a + b).flatMap(k => List(k.size))<console>:27: error: value size is not a member of Char       name1.reduce((a,b) => a + b).flatMap(k => List(k.size))                                                        ^scala> name1.flatMap(k => List(k.size)).reduce((a,b) => a + b)res68: Int = 13     // AppleIBMYahoo.size = 13scala> name1.flatMap(k => List(k.size)).reduce((a,b) => a + b)res68: Int = 13//calculate string’size and then reducescala> val name2 = sc.parallelize(List("Apple","Yahoo","IBM")).map(x => (x,x.size))name2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[179] at map at <console>:24scala> name2.flatMap(k => Array(k._2)).reduce(_+_)res71: Int = 13scala> name1.flatMap(k => List(k.size)).reduce(_+_)res72: Int = 13//map first, and then reduce ()scala> name1.map(str => (str, str.size)).flatMap(x => Array(x._2)).reduce(_+_)res74: Int = 13//Adding of map _._2 directly will be errorscala> name1.map(str => (str, str.size)).reduce(_._2+_._2)<console>:27: error: type mismatch; found   : Int required: (String, Int)       name1.map(str => (str, str.size)).reduce(_._2+_._2)//About Key-Pairscala> name2.collectres75: Array[(String, Int)] = Array((Apple,5), (Yahoo,5), (IBM,3))scala> name2.firstres76: (String, Int) = (Apple,5)scala> name2.take(2)res77: Array[(String, Int)] = Array((Apple,5), (Yahoo,5))//Samplescala> val pool = sc.parallelize(List("M","Tu","W","Tr","F"))pool: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[184] at parallelize at <console>:24scala> pool.takeSample(true,3)res80: Array[String] = Array(F, M, F)scala> pool.takeSample(true,3)res81: Array[String] = Array(F, W, M)scala> pool.takeSample(true,3)res82: Array[String] = Array(Tu, F, M)scala> pool.takeSample(true,3)res83: Array[String] = Array(F, Tu, M)//countByKey is also Actionscala> pool.map(k => (k,1)).countByKeyres85: scala.collection.Map[String,Long] = Map(Tr -> 1, F -> 1, M -> 1, Tu -> 1, W -> 1)