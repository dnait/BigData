makeRDD VS parallelize

makeRDD is identical to parallelize. And if you take a look at the implementation it simply calls parallelize:

def makeRDD[T: ClassTag](
    seq: Seq[T],
    numSlices: Int = defaultParallelism): RDD[T] = withScope {
  parallelize(seq, numSlices)
}
At the end of the day it is a matter of taste. One thing to consider is that makeRDD seems to be specific to Scala API. PySpark and internal SparkR API provide only parallelize.

Note: There is a second implementation of makeRDD which allows you to set location preferences, but given a different signature it is not interchangeable with parallelize.

examples:

scala> val seq = List((1, List("yahoo1","ibm2","google3")), (2, List("yahoo1","google3")))seq: List[(Int, List[String])] = List((1,List(yahoo1, ibm2, google3)), (2,List(yahoo1, google3)))scala> val res = sc.makeRDD(seq)res: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at <console>:26

scala> val res1 = sc.parallelize(seq)
res1: org.apache.spark.rdd.RDD[(Int, List[String])] = ParallelCollectionRDD[13] at parallelize at <console>:26scala> res.preferredLocations(res.partitions(0))res9: Seq[String] = List(yahoo1, ibm2, google3)scala> res.preferredLocations(res.partitions(9))res10: Seq[String] = List(yahoo1, google3)scala> val res = sc.parallelize(List(1,2,3))res: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at <console>:24scala> res.preferredLocations(res.partitions(0))res11: Seq[String] = List()scala> res.preferredLocations(res.partitions(1))res12: Seq[String] = List()