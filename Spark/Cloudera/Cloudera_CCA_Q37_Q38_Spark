Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

Q37 join and break nest List

scala> val manager = sc.textFile("spark1/EmployeeManager.csv")
manager: org.apache.spark.rdd.RDD[String] = spark1/EmployeeManager.csv MapPartitionsRDD[1] at textFile at <console>:24


scala> val managerRDD = manager.map(x=> (x.split(","))(0), x.split(",")(1)))
<console>:1: error: ';' expected but ')' found.
val managerRDD = manager.map(x=> (x.split(","))(0), x.split(",")(1)))
                                                                    ^

scala> val managerRDD = manager.map(x=> (x.split(",")(0), x.split(",")(1)))
managerRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[2] at map at <console>:26

scala> managerRDD.collect
res0: Array[(String, String)] = Array((1,Miles), (2,Quark), (3,Keiko), (4,Julian), (5,Ben), (6,Julian), (7,Leeta), (8,Martok), (9,Nog), (10,Keiko))

scala> val salary = sc.textFile("spark1/EmployeeSalary.csv")
salary: org.apache.spark.rdd.RDD[String] = spark1/EmployeeSalary.csv MapPartitionsRDD[4] at textFile at <console>:24

scala> val salaryRDD = salary.map(x=>(x.split(",")(0), x.split(",")(1)))
salaryRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[5] at map at <console>:26

scala> val nameRDD = sc.textFile("spark1/EmployeeName.csv").map(x=>(x.split(",")(0), x.split(",")(1)))
nameRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[8] at map at <console>:24

scala> val join = nameRDD.join(salaryRDD).join(managerRDD)
join: org.apache.spark.rdd.RDD[(String, ((String, String), String))] = MapPartitionsRDD[14] at join at <console>:34

scala> join.collect
res1: Array[(String, ((String, String), String))] = Array((4,((Quark,21000),Julian)), (8,((Jadzia,38000),Martok)), (6,((Gowron,220000),Julian)), (2,((Hugh,22100),Quark)), (7,((Will,307000),Leeta)), (5,((Weyoun,318000),Ben)), (9,((Hugh,18100),Nog)), (3,((Deanna,4650),Keiko)), (1,((Jean-Luc,2000),Miles)), (10,((Odo,19100),Keiko)))

scala> val joinData = join.sortByKey
<console>:36: error: missing argument list for method sortByKey in class OrderedRDDFunctions
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `sortByKey _` or `sortByKey(_,_)` instead of `sortByKey`.
       val joinData = join.sortByKey
                           ^

scala> val joinData = join.sortByKey()
joinData: org.apache.spark.rdd.RDD[(String, ((String, String), String))] = ShuffledRDD[17] at sortByKey at <console>:36

scala> joinData.collect
res2: Array[(String, ((String, String), String))] = Array((1,((Jean-Luc,2000),Miles)), (10,((Odo,19100),Keiko)), (2,((Hugh,22100),Quark)), (3,((Deanna,4650),Keiko)), (4,((Quark,21000),Julian)), (5,((Weyoun,318000),Ben)), (6,((Gowron,220000),Julian)), (7,((Will,307000),Leeta)), (8,((Jadzia,38000),Martok)), (9,((Hugh,18100),Nog)))

scala> val finalData = joinData.map(v=> (v._1,v._2._1._1,v._2._1._2,v._2._2))
finalData: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[18] at map at <console>:38

scala> finalData.collect
res3: Array[(String, String, String, String)] = Array((1,Jean-Luc,2000,Miles), (10,Odo,19100,Keiko), (2,Hugh,22100,Quark), (3,Deanna,4650,Keiko), (4,Quark,21000,Julian), (5,Weyoun,318000,Ben), (6,Gowron,220000,Julian), (7,Will,307000,Leeta), (8,Jadzia,38000,Martok), (9,Hugh,18100,Nog))

scala> finalData.saveAsTextFile("spark1/q37res.txt")

scala> 




Q38 Using broadcast to filter words in the black list

scala> val words = sc.textFile("spark1/Content.txt").flatMap(_.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at flatMap at <console>:24

scala> val removeRdd = sc.textFile("spark1/remove.txt").flatMap(_.split(",")).map(_.trim)
removeRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at map at <console>:24

scala> val bRemove = sc.broadcast(removeRdd.collect().toList)
bRemove: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(7)

scala> val filtered = words.filter{case (word) => !bRemove.value.contains(word)} 
filtered: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at filter at <console>:30

scala> filtered.collect
res1: Array[String] = Array(Jingle, bells, jingle, bells, jingle, all, way, Oh, what, fun, it, is, to, ride, in, one, horse, open, sleigh, Ladies, and, gentlemen, I, give, you, jingle, bass, Merry, Christmas, Dashing, through, snow, On, one, horse, open, sleigh, O, er, fields, we, go, Laughing, all, way, Bells, on, bobtail, ring, Making, spirits, bright, What, fun, it, is, to, ride, and, sing, sleighing, song, tonight, Jingle, bells, jingle, bells, jingle, all, way, Oh, what, fun, it, is, to, ride, in, one, horse, open, sleigh, Jingle, bells, jingle, bells, jingle, all, way, Oh, what, fun, it, is, to, ride, in, one, horse, open, sleigh, Let, me, hear, you, say, What, fun, it, is, to, sing, sleighing, song, tonight, Dashing, through, snow, On, one, horse, open, sleigh, O, er, fields, we,...
scala> val wordCount = filtered.map(x => (x,1)).reduceByKey(_+_)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at <console>:32

scala> wordCount.collect
res2: Array[(String, Int)] = Array((hear,1), (Bells,2), (spirits,1), (is,5), (bells,6), (bright,1), (ride,4), (Christmas,1), (one,5), (we,2), (Dashing,2), (ring,2), (Ladies,1), (What,2), (go,2), (what,3), (sleighing,2), (open,5), (sleigh,5), (me,1), (Laughing,2), (jingle,7), (Let,1), (it,5), (Making,1), (you,2), (song,2), (sing,2), (bobtail,2), (on,2), (say,1), (O,2), (bass,1), (fields,2), (all,5), (through,2), (I,1), (to,5), (in,3), (Oh,3), (gentlemen,1), (Jingle,3), (tonight,2), (er,2), (horse,5), (fun,5), (and,2), (snow,2), (On,2), (Merry,1), (give,1), (way,5))

scala> wordCount.saveAsTextFile("spark1/Q38res.txt")

scala> 

